runs <- 10000  # numero de iteraciones bootstrap para los intervalos de confianza
crit <- "FPE(n)" # criterio con cual elegir el orden inicial del VAR(p)
confianza <- .95
}
source(paste0(path, "Funciones_VARPLSParallel.R"))# cargar funciones auxiliares
##########################################
# Lectura de datos                       #
# Se espera un dataframe donde la primer columna sean las fechas de las series y la segunda la variable de interes a pronosticar
data <- read.csv(paste0(path, "Compendio13Mayo2019.csv"), row.names = 1)
##########################################
# imputacion de datos                    #
data <- na.omit(data)
temp <- row.names(data)
data <- as.data.frame(sapply(data, log))
row.names(data) <- temp
data2 <- data
#data[, 3:17] <- NULL
##########################################
# visualizacion
data.s <- data
data.s$time <- row.names(data)
data.s$time <- dmy(data.s$time)
data.s <- melt(data.s, id='time')
ggplot(data.s, aes(x= time, y =exp(value), color=variable)) + geom_line() +
facet_wrap(variable~., scales = "free") +  theme_minimal() + xlab('') +
ylab('') +  theme(legend.position = "bottom", legend.title = element_text(color = "white")) +
ggtitle('Variables econometr?cas') +guides( color=FALSE)
###########################################
fechas <- row.names(data) # guardamos las fechas
row.names(data) <- fechas
# division de la muestra como sugiere Frances
n <- dim(data)[1] # tamaño total de la muestra
k <- dim(data)[2] # numero de componentes
Y <- tail(data, n - h + 1 )
X <- head(data, n - h + 1 )
# test de cointegracion de
CarloMagno(data, lag.max=lag.max)
p <- VARselect(y= X, lag.max = lag.max)$selection[crit]# determinacion del orden del VAR(p)
Y <- Y[ dim(Y)[1]:1, ] # Frances propone este acomodo de las observaciones
X <- X[ dim(X)[1]:1, ]
Y <- as.data.frame(Y)
X <- as.data.frame(X)
colnames(Y) <- colnames(X) <- colnames(data)
X.lags <- SpanMatrix(X, p=(p))# generamos la matriz extendida con todos los lags
Y.lags <- SpanMatrix(X, p=(h-1))# generamos la matriz extendida con todos los lags
model <- plsr(Y.lags~ X.lags, method = "oscorespls", x=TRUE, y=TRUE)
componentes.practicas <- model$ncomp # por los nulos se reducen
Pronosticos <- mclapply(FUN=function(x) Predict.PLS(modelo=model, original=Y, ncomp=x, h=h),
1:componentes.practicas)
temp1 <- temp <- matrix(nrow = componentes.practicas, ncol = dim(data)[2])
temp1 <- temp <- as.data.frame(temp)
colnames(temp1) <- colnames(temp) <- colnames(data)
row.names(temp1) <- row.names(temp) <- paste0('Comp.', 1:componentes.practicas)
for(i in 1:componentes.practicas)
{
temp[i, ] <- Error.relativo(Pronosticos[[i]], ncomp = i, data, h)
temp1[i, ] <- MAPE(x=Pronosticos[[i]], ncomp=i, data, h)
}
temp1$id <- 1:componentes.practicas
Ncomp <- which.min(temp[, 1])
}
###########################################
# librerias                               #
{
library(vars) # tiene varias dependencias (implicitamente carga en el ambiente otras) util para modelos VAR
library(pls)  # para estimacion pls
library(psych) # solo ocupamos una funcion de aqui CHECAR CUAL ES
library(ggplot2) # libreria de graficos
library(lubridate)  # libreria para manejo de fechas
library(reshape2) #manipulacion de dataframes
library(parallel)
}
path <- '/home/fou/Desktop/MCE2/4/Tesina/Code/' # ubicacion del archivo 'Funciones_VARPLSParallel.R' y los datos
h <- 6 # numero de steps a pronostricar
lag.max <- 6 # lag maximo para la determinacion inicial del AR(p)
runs <- 10000  # numero de iteraciones bootstrap para los intervalos de confianza
crit <- "FPE(n)" # criterio con cual elegir el orden inicial del VAR(p)
confianza <- .95
source(paste0(path, "Funciones_VARPLSParallel.R"))# cargar funciones auxiliares
##########################################
# Lectura de datos                       #
# Se espera un dataframe donde la primer columna sean las fechas de las series y la segunda la variable de interes a pronosticar
data <- read.csv(paste0(path, "Compendio13Mayo2019.csv"), row.names = 1)
##########################################
# imputacion de datos                    #
data <- na.omit(data)
temp <- row.names(data)
data <- as.data.frame(sapply(data, log))
row.names(data) <- temp
data2 <- data
#data[, 3:17] <- NULL
##########################################
# visualizacion
data.s <- data
data.s$time <- row.names(data)
data.s$time <- dmy(data.s$time)
data.s <- melt(data.s, id='time')
ggplot(data.s, aes(x= time, y =exp(value), color=variable)) + geom_line() +
facet_wrap(variable~., scales = "free") +  theme_minimal() + xlab('') +
ylab('') +  theme(legend.position = "bottom", legend.title = element_text(color = "white")) +
ggtitle('Variables econometr?cas') +guides( color=FALSE)
###########################################
fechas <- row.names(data) # guardamos las fechas
row.names(data) <- fechas
# division de la muestra como sugiere Frances
n <- dim(data)[1] # tamaño total de la muestra
k <- dim(data)[2] # numero de componentes
Y <- tail(data, n - h + 1 )
X <- head(data, n - h + 1 )
# test de cointegracion de
CarloMagno(data, lag.max=lag.max)
p <- VARselect(y= X, lag.max = lag.max)$selection[crit]# determinacion del orden del VAR(p)
Y <- Y[ dim(Y)[1]:1, ] # Frances propone este acomodo de las observaciones
X <- X[ dim(X)[1]:1, ]
Y <- as.data.frame(Y)
X <- as.data.frame(X)
colnames(Y) <- colnames(X) <- colnames(data)
X.lags <- SpanMatrix(X, p=(p))# generamos la matriz extendida con todos los lags
Y.lags <- SpanMatrix(X, p=(h-1))# generamos la matriz extendida con todos los lags
model <- plsr(Y.lags~ X.lags, method = "oscorespls", x=TRUE, y=TRUE)
componentes.practicas <- model$ncomp # por los nulos se reducen
Pronosticos <- mclapply(FUN=function(x) Predict.PLS(modelo=model, original=Y, ncomp=x, h=h),
1:componentes.practicas)
temp1 <- temp <- matrix(nrow = componentes.practicas, ncol = dim(data)[2])
temp1 <- temp <- as.data.frame(temp)
colnames(temp1) <- colnames(temp) <- colnames(data)
row.names(temp1) <- row.names(temp) <- paste0('Comp.', 1:componentes.practicas)
for(i in 1:componentes.practicas)
{
temp[i, ] <- Error.relativo(Pronosticos[[i]], ncomp = i, data, h)
temp1[i, ] <- MAPE(x=Pronosticos[[i]], ncomp=i, data, h)
}
temp1$id <- 1:componentes.practicas
Ncomp <- which.min(temp[, 1])
{
###########################################
# librerias                               #
{
library(vars) # tiene varias dependencias (implicitamente carga en el ambiente otras) util para modelos VAR
library(pls)  # para estimacion pls
library(psych) # solo ocupamos una funcion de aqui CHECAR CUAL ES
library(ggplot2) # libreria de graficos
library(lubridate)  # libreria para manejo de fechas
library(reshape2) #manipulacion de dataframes
library(parallel)
}
###########################################
# Parametros                              #
{
path <- '/home/fou/Desktop/MCE2/4/Tesina/Code/' # ubicacion del archivo 'Funciones_VARPLSParallel.R' y los datos
h <- 6 # numero de steps a pronostricar
lag.max <- 6 # lag maximo para la determinacion inicial del AR(p)
runs <- 10000  # numero de iteraciones bootstrap para los intervalos de confianza
crit <- "FPE(n)" # criterio con cual elegir el orden inicial del VAR(p)
confianza <- .95
}
source(paste0(path, "Funciones_VARPLSParallel.R"))# cargar funciones auxiliares
##########################################
# Lectura de datos                       #
# Se espera un dataframe donde la primer columna sean las fechas de las series y la segunda la variable de interes a pronosticar
data <- read.csv(paste0(path, "Compendio13Mayo2019.csv"), row.names = 1)
##########################################
# imputacion de datos                    #
data <- na.omit(data)
temp <- row.names(data)
data <- as.data.frame(sapply(data, log))
row.names(data) <- temp
data2 <- data
#data[, 3:17] <- NULL
##########################################
# visualizacion
data.s <- data
data.s$time <- row.names(data)
data.s$time <- dmy(data.s$time)
data.s <- melt(data.s, id='time')
ggplot(data.s, aes(x= time, y =exp(value), color=variable)) + geom_line() +
facet_wrap(variable~., scales = "free") +  theme_minimal() + xlab('') +
ylab('') +  theme(legend.position = "bottom", legend.title = element_text(color = "white")) +
ggtitle('Variables econometr?cas') +guides( color=FALSE)
###########################################
fechas <- row.names(data) # guardamos las fechas
row.names(data) <- fechas
# division de la muestra como sugiere Frances
n <- dim(data)[1] # tamaño total de la muestra
k <- dim(data)[2] # numero de componentes
Y <- tail(data, n - h + 1 )
X <- head(data, n - h + 1 )
# test de cointegracion de
CarloMagno(data, lag.max=lag.max)
p <- VARselect(y= X, lag.max = lag.max)$selection[crit]# determinacion del orden del VAR(p)
Y <- Y[ dim(Y)[1]:1, ] # Frances propone este acomodo de las observaciones
X <- X[ dim(X)[1]:1, ]
Y <- as.data.frame(Y)
X <- as.data.frame(X)
colnames(Y) <- colnames(X) <- colnames(data)
X.lags <- SpanMatrix(X, p=(p))# generamos la matriz extendida con todos los lags
Y.lags <- SpanMatrix(X, p=(h-1))# generamos la matriz extendida con todos los lags
model <- plsr(Y.lags~ X.lags, method = "oscorespls", x=TRUE, y=TRUE)
componentes.practicas <- model$ncomp # por los nulos se reducen
Pronosticos <- mclapply(FUN=function(x) Predict.PLS(modelo=model, original=Y, ncomp=x, h=h),
1:componentes.practicas)
temp1 <- temp <- matrix(nrow = componentes.practicas, ncol = dim(data)[2])
temp1 <- temp <- as.data.frame(temp)
colnames(temp1) <- colnames(temp) <- colnames(data)
row.names(temp1) <- row.names(temp) <- paste0('Comp.', 1:componentes.practicas)
for(i in 1:componentes.practicas)
{
temp[i, ] <- Error.relativo(Pronosticos[[i]], ncomp = i, data, h)
temp1[i, ] <- MAPE(x=Pronosticos[[i]], ncomp=i, data, h)
}
temp1$id <- 1:componentes.practicas
Ncomp <- which.min(temp[, 1])
}
###########################################
# librerias                               #
{
library(vars) # tiene varias dependencias (implicitamente carga en el ambiente otras) util para modelos VAR
library(pls)  # para estimacion pls
library(psych) # solo ocupamos una funcion de aqui CHECAR CUAL ES
library(ggplot2) # libreria de graficos
library(lubridate)  # libreria para manejo de fechas
library(reshape2) #manipulacion de dataframes
library(parallel)
}
###########################################
# Parametros                              #
{
path <- '/home/fou/Desktop/MCE2/4/Tesina/Code/' # ubicacion del archivo 'Funciones_VARPLSParallel.R' y los datos
h <- 6 # numero de steps a pronostricar
lag.max <- 6 # lag maximo para la determinacion inicial del AR(p)
runs <- 10000  # numero de iteraciones bootstrap para los intervalos de confianza
crit <- "FPE(n)" # criterio con cual elegir el orden inicial del VAR(p)
confianza <- .95
}
source(paste0(path, "Funciones_VARPLSParallel.R"))# cargar funciones auxiliares
##########################################
# Lectura de datos                       #
# Se espera un dataframe donde la primer columna sean las fechas de las series y la segunda la variable de interes a pronosticar
data <- read.csv(paste0(path, "Compendio13Mayo2019.csv"), row.names = 1)
##########################################
# imputacion de datos                    #
data <- na.omit(data)
temp <- row.names(data)
data <- as.data.frame(sapply(data, log))
row.names(data) <- temp
data2 <- data
#data[, 3:17] <- NULL
##########################################
# visualizacion
data.s <- data
data.s$time <- row.names(data)
data.s$time <- dmy(data.s$time)
data.s <- melt(data.s, id='time')
ggplot(data.s, aes(x= time, y =exp(value), color=variable)) + geom_line() +
facet_wrap(variable~., scales = "free") +  theme_minimal() + xlab('') +
ylab('') +  theme(legend.position = "bottom", legend.title = element_text(color = "white")) +
ggtitle('Variables econometr?cas') +guides( color=FALSE)
###########################################
fechas <- row.names(data) # guardamos las fechas
row.names(data) <- fechas
# division de la muestra como sugiere Frances
n <- dim(data)[1] # tamaño total de la muestra
k <- dim(data)[2] # numero de componentes
Y <- tail(data, n - h + 1 )
X <- head(data, n - h + 1 )
# test de cointegracion de
CarloMagno(data, lag.max=lag.max)
p <- VARselect(y= X, lag.max = lag.max)$selection[crit]# determinacion del orden del VAR(p)
Y <- Y[ dim(Y)[1]:1, ] # Frances propone este acomodo de las observaciones
X <- X[ dim(X)[1]:1, ]
Y <- as.data.frame(Y)
X <- as.data.frame(X)
colnames(Y) <- colnames(X) <- colnames(data)
X.lags <- SpanMatrix(X, p=(p))# generamos la matriz extendida con todos los lags
Y.lags <- SpanMatrix(X, p=(h-1))# generamos la matriz extendida con todos los lags
model <- plsr(Y.lags~ X.lags, method = "oscorespls", x=TRUE, y=TRUE)
componentes.practicas <- model$ncomp # por los nulos se reducen
Pronosticos <- mclapply(FUN=function(x) Predict.PLS(modelo=model, original=Y, ncomp=x, h=h),
1:componentes.practicas)
temp1 <- temp <- matrix(nrow = componentes.practicas, ncol = dim(data)[2])
temp1 <- temp <- as.data.frame(temp)
colnames(temp1) <- colnames(temp) <- colnames(data)
row.names(temp1) <- row.names(temp) <- paste0('Comp.', 1:componentes.practicas)
for(i in 1:componentes.practicas)
{
temp[i, ] <- Error.relativo(Pronosticos[[i]], ncomp = i, data, h)
temp1[i, ] <- MAPE(x=Pronosticos[[i]], ncomp=i, data, h)
}
temp1$id <- 1:componentes.practicas
Ncomp <- which.min(temp[, 1])
# grafica de mape
z <- melt(temp1, id='id')
Ncomp.mape <- which.min(temp1[, 1])
ggplot(z, aes(x=id, y=value, color=variable)) + geom_line() +
facet_wrap(variable~., scales = "free") +  theme_minimal() + xlab('') +
ylab('') +  theme(legend.position = "bottom", legend.title = element_text(color = "white")) +
ggtitle('MAPE') +guides( color=FALSE)
###################
# intervalos de confianza
set.seed(0)
Intervalos <- mclapply(rep(p, runs), function(x) Bootstrap(x=X, X.lags = X.lags, p=x, Y=Y, Y.lags = Y.lags, Ncomp.mape=Ncomp.mape)  )
intervalos <- do.call('rbind', Intervalos)
intervalos <- as.data.frame(intervalos)
significancia <- (1 - confianza)/2
t <- Sys.time()
c.i <- lapply(intervalos, function(x)quantile(x, probs=c(significancia, 1-significancia )) )
c.i <- as.data.frame(do.call('rbind', c.i))
c.i$l <- c.i[, 2] -c.i[, 1]
#######################################################
### estimacion del VAR a comparar
var <- VAR(ts(data2, start = c(2015, 1), frequency = 12), p=p, ic='FPE',
lag.max = lag.max )
tabla.var <- data.frame(Pronostico=predict(var, n.ahead = h)$fcst$InflacionNacional[,'fcst'],
y = tail(data2[,1],h ))
tabla.var$Error.relativo <- abs(tabla.var$y - tabla.var$Pronostico)/abs(tabla.var$Pronostico)*100
tabla.var
mean(tabla.var$Error.relativo)
######################################
# plot de pronostico conjunto
Final <- data.frame(Valor.Real=tail(data2[,1], h) ,
VAR.PLS = Predict.PLS(modelo=model, original=Y, h = h, ncomp=Ncomp.mape)[,1],
VAR=predict(var, n.ahead = h)$fcst$InflacionNacional[,'fcst'])
tabla <- Final <- exp(Final)
Final <- cbind(Final, c.i[, 1:2])
Final[, 4] <- Final$VAR.PLS - Final[, 4]
Final[, 5] <- Final$VAR.PLS + Final[, 5]
Final$t <- dmy(row.names(tail(data, h)))
data <- exp(data2)
data$t <- dmy(row.names(data))
t <- melt(Final, id='t')
tabla$Error.relativo.pls <- abs(tabla$Valor.Real - tabla$VAR.PLS )/abs(tabla$Valor.Real)*100
tabla$Error.relativo.var <- abs(tabla$Valor.Real - tabla$VAR )/abs(tabla$Valor.Real)*100
tabla
tabla$Presicio.VAR.PLS <- 100 - tabla$Error.relativo.pls
tabla$Presicio.VAR <- 100 - tabla$Error.relativo.var
library(xtable)
xtable(tabla)
sapply(tabla, mean)
xtable(cbind(c.i, tabla[,2]))
tabla
ggplot(data = data, aes(y=InflacionNacional, x=t)) + geom_line() +
geom_line(data=subset(t, variable != 'VAR'), aes(x=t, y=value, color=variable)) + theme_minimal() +
ylab('') + xlab('') +xlim(ymd('2017-01-01'), max(data$t)) +
ylim(c(90,max(c(data$InflacionNacional, t$value )))) + guides( color=FALSE) +
ggtitle(paste0('Pron?stico VAR(', p, ')-PLS(h=', h, ', k=', Ncomp.mape, ')'))  +
scale_color_manual(values = c('Valor.Real'= 'navy', 'VAR.PLS' = 'red',
'2.5%' = 'orange', '97.5%' = 'orange'))
ggplot(data = data, aes(y=InflacionNacional, x=t)) + geom_line() +
geom_line(data=subset(t, variable != 'VAR'), aes(x=t, y=value, color=variable)) + theme_minimal() +
ylab('') + xlab('') +xlim(ymd('2017-01-01'), max(data$t)) +
ylim(c(90,max(c(data$InflacionNacional, t$value )))) + guides( color=FALSE) +
ggtitle(paste0('Pronóstico VAR(', p, ')-PLS(h=', h, ', k=', Ncomp.mape, ')'))  +
scale_color_manual(values = c('Valor.Real'= 'navy', 'VAR.PLS' = 'red',
'2.5%' = 'orange', '97.5%' = 'orange'))
ggplot(data = data, aes(y=InflacionNacional, x=t)) + geom_line() +
geom_line(data=subset(t, variable != 'VAR'), aes(x=t, y=value, color=variable)) + theme_minimal() +
ylab('') + xlab('') +xlim(ymd('2017-06-01'), max(data$t)) +
ylim(c(90,max(c(data$InflacionNacional, t$value )))) + guides( color=FALSE) +
ggtitle(paste0('Pronóstico VAR(', p, ')-PLS(h=', h, ', k=', Ncomp.mape, ')'))  +
scale_color_manual(values = c('Valor.Real'= 'navy', 'VAR.PLS' = 'red',
'2.5%' = 'orange', '97.5%' = 'orange'))
ggplot(data = data, aes(y=InflacionNacional, x=t)) + geom_line() +
geom_line(data=subset(t, variable != 'VAR'), aes(x=t, y=value, color=variable)) + theme_minimal() +
ylab('') + xlab('') +xlim(ymd('2017-06-01'), max(data$t)) +
ylim(c(92,max(c(data$InflacionNacional, t$value )))) + guides( color=FALSE) +
ggtitle(paste0('Pronóstico VAR(', p, ')-PLS(h=', h, ', k=', Ncomp.mape, ')'))  +
scale_color_manual(values = c('Valor.Real'= 'navy', 'VAR.PLS' = 'red',
'2.5%' = 'orange', '97.5%' = 'orange'))
#####################################################################
# Actualizacion en el servidor de CIMAT 30 de mayo de abril de 2019 #
# J. Antonio García Ramirez  jose.antonio@cimat.mx               ####
#####################################################################
# Programa para ejecutar todos los pronosticos y graficas HASTA DICIEMBRE DE 2018
# que se emplean en la app disponible en http://10.14.10.84:3838/obsPronostico/
# de la red interna de CIMAT para el INPC
#####################################################################
# Carga de librerias
{
remove(list=ls())
t.inicio <- Sys.time() # medimos el tiempo de ejecucion
library(pls)    # estimacion de minimos cuadrados parciales
library(lubridate) # manejo sencillo de fechas
library(vars) # funciones para estimar modelos var
library(forecast) # unknown
dt.file <-"/home/fou/Desktop/ObsPronostico/obsPronostico" # direccion en dond se encuentra la carpeta 'obsPronostico' en el servidor (es el ambiente de desarollo NO LA DE PRODUCCION)
source(paste0(dt.file, "model_functions.r", sep ="")) # define funciones para chechar cointegracion, seleccion y validacion de modelos VAR
source(paste(dt.file, "inflacionMax.r", sep ="")) # define una unica funcion que selecciona el mejor modelo y escribe en disco archivos
}
#####################################################################
# Actualizacion en el servidor de CIMAT 30 de mayo de abril de 2019 #
# J. Antonio García Ramirez  jose.antonio@cimat.mx               ####
#####################################################################
# Programa para ejecutar todos los pronosticos y graficas HASTA DICIEMBRE DE 2018
# que se emplean en la app disponible en http://10.14.10.84:3838/obsPronostico/
# de la red interna de CIMAT para el INPC
#####################################################################
# Carga de librerias
{
remove(list=ls())
t.inicio <- Sys.time() # medimos el tiempo de ejecucion
library(pls)    # estimacion de minimos cuadrados parciales
library(lubridate) # manejo sencillo de fechas
library(vars) # funciones para estimar modelos var
library(forecast) # unknown
dt.file <-"/home/fou/Desktop/ObsPronostico/obsPronostico" # direccion en dond se encuentra la carpeta 'obsPronostico' en el servidor (es el ambiente de desarollo NO LA DE PRODUCCION)
source(paste0(dt.file, "model_functions.r", sep ="")) # define funciones para chechar cointegracion, seleccion y validacion de modelos VAR
source(paste(dt.file, "inflacionMax.r", sep ="")) # define una unica funcion que selecciona el mejor modelo y escribe en disco archivos
}
#####################################################################
# Actualizacion en el servidor de CIMAT 30 de mayo de abril de 2019 #
# J. Antonio García Ramirez  jose.antonio@cimat.mx               ####
#####################################################################
# Programa para ejecutar todos los pronosticos y graficas HASTA DICIEMBRE DE 2018
# que se emplean en la app disponible en http://10.14.10.84:3838/obsPronostico/
# de la red interna de CIMAT para el INPC
#####################################################################
# Carga de librerias
{
remove(list=ls())
t.inicio <- Sys.time() # medimos el tiempo de ejecucion
library(pls)    # estimacion de minimos cuadrados parciales
library(lubridate) # manejo sencillo de fechas
library(vars) # funciones para estimar modelos var
library(forecast) # unknown
dt.file <-"/home/fou/Desktop/ObsPronostico/obsPronostico/" # direccion en dond se encuentra la carpeta 'obsPronostico' en el servidor (es el ambiente de desarollo NO LA DE PRODUCCION)
source(paste0(dt.file, "model_functions.r", sep ="")) # define funciones para chechar cointegracion, seleccion y validacion de modelos VAR
source(paste(dt.file, "inflacionMax.r", sep ="")) # define una unica funcion que selecciona el mejor modelo y escribe en disco archivos
}
#####################################################################
# Actualizacion en el servidor de CIMAT 30 de mayo de abril de 2019 #
# J. Antonio García Ramirez  jose.antonio@cimat.mx               ####
#####################################################################
# Programa para ejecutar todos los pronosticos y graficas HASTA DICIEMBRE DE 2018
# que se emplean en la app disponible en http://10.14.10.84:3838/obsPronostico/
# de la red interna de CIMAT para el INPC
#####################################################################
# Carga de librerias
{
remove(list=ls())
t.inicio <- Sys.time() # medimos el tiempo de ejecucion
library(pls)    # estimacion de minimos cuadrados parciales
library(lubridate) # manejo sencillo de fechas
library(vars) # funciones para estimar modelos var
library(forecast) # unknown
dt.file <-"/home/fou/Desktop/ObsPronostico/obsPronostico/" # direccion en dond se encuentra la carpeta 'obsPronostico' en el servidor (es el ambiente de desarollo NO LA DE PRODUCCION)
source(paste0(dt.file, "model_functions.r", sep ="")) # define funciones para chechar cointegracion, seleccion y validacion de modelos VAR
source(paste(dt.file, "inflacionMax.r", sep ="")) # define una unica funcion que selecciona el mejor modelo y escribe en disco archivos
}
#####################################################################
# lectura de DATOS de las 24 series de tiempo
#####################################################################
# el contenido de cada archivo de lectura se encuentra en la documentacion
# IMPORTANTE: mantener la estructura y nombre de archivos que se menciona en la documentacion
{
monetario <- read.csv(paste(dt.file, "/Econ.DataMax/Monetario.csv", sep = ""), row.names = 1)
row.names(monetario) <- as.character(dmy(row.names(monetario)))
monetario <- read.csv(paste(dt.file, "/Econ.DataMax/Monetario.csv", sep = ""), row.names = 1)
row.names(monetario) <- as.character(dmy(row.names(monetario)))
costos <- read.csv(paste(dt.file, "/Econ.DataMax/Costos.csv", sep = ""), row.names = 1)
row.names(costos) <- as.character(dmy(row.names(costos)))
demanda <- read.csv(paste(dt.file, "/Econ.DataMax/Demanda.csv", sep = ""), row.names = 1)
row.names(demanda) <- as.character(dmy(row.names(demanda)))
precios <- read.csv(paste(dt.file, "/Econ.DataMax/Precios.csv", sep = ""), row.names = 1)
row.names(precios) <- as.character(dmy(row.names(precios)))
precios.row <- precios
}
####################################################################
#  PARAMETROS
####################################################################
# Se fijan los parametros a valores ya probados.
# para valores de parametros que produzcan errores, la documentación indica que valores usar
{
args <- commandArgs(TRUE) # recepcion de los valores de la consola de linux
print(c(args))
length.fore <- strtoi(args[1])  # Num. de meses a pronosticar
mes.shiny <- strtoi(args[2]) # ano a pronosticar
print(paste0('Número de meses a pronosticar ',length.fore ))
print(paste0('Mes a pronosticar ', mes.shiny))
regiones <- c("Frontera.norte", "Noroeste", "Noreste", "Centro.norte",
"Centro.sur", "Sur", "Mexico", "Nacional") # regiones economicas del pais (ver documentacion)
print(paste0('Número de meses a pronosticar ',length.fore ))
print(paste0('Mes a pronosticar ', mes.shiny))
c.sig <- 0.10      # Nivel de significancia
show.data <- 48    # ventana de tiempo
length.test <- 6    # Meses a probar  intramuestra
n.try <- 5         # Rezagos a probar
restrict <- FALSE  # TRUE Si pronostico no puede superar (min,max)
objective <- 3     # Lo usa en el bias - Ahora el objetivo de BM es 3
lag.max <- 2    # Para el numero de modelos
seas.max <- 2    # Para el numero de modelos
anual <- 12             # Para tasa interanual >> lo añadio Andres
ec.det <- c("none", "const", "trend")
anio.parser <- c(paste0('0',1:9), 10:12)
names(anio.parser) <- c('Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio',
'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre')
anio.loop <- as.character(2017:2018) # anos en los que el usuario puede consultar en el dashboard
}
anio.shiny <- anio.loop[1]
mes.shiny <- 1
# se itera sobre todos los meses del ano
mes.first <- dmy("01/01/2005")
mes.last <- dmy(paste0('01/', mes.shiny, '/', anio.shiny))
# se itera sobre todos los meses del ano
mes.first <- dmy("01/01/2005")
mes.last <- dmy(paste0('01/', mes.shiny, '/', anio.shiny))
mes.format <- as.Date(mes.last, format = "%d/%m/%Y")
dir.create(paste0(dt.file,'resultados_boletinMax/', mes.format, '/')) # se crea la carpeta en donde se depositan los archivos necesarios para visualizar
# el promedio de la tasa interanual para el INPC
setwd(paste0(dt.file,'resultados_boletinMax/', mes.format))
# LOOP sobre las regiones
pronostico_regional <- as.data.frame(matrix(NA, nrow=length.fore+1))
pronostico_tasa_mensual <- as.data.frame(matrix(NA, nrow=length.fore+1))
